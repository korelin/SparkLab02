{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.io.Source\n",
    "import scala.util.Try\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer,StopWordsRemover}\n",
    "import org.apache.spark.ml.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix, RowMatrix}\n",
    "import org.apache.spark.sql.{SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.log4j.{Logger, Level}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\n",
       "org.apache.spark.SparkContext.<init>(SparkContext.scala:76)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:46)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:54)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:56)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:58)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:60)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:62)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:64)\n",
       "$line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:66)\n",
       "$line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:68)\n",
       "$line14.$read$$iw$$iw$$iw.<init>(<console>:70)\n",
       "$line14.$read$$iw$$iw.<init>(<console>:72)\n",
       "$line14.$read$$iw.<init>(<console>:74)\n",
       "$line14.$read.<init>(<console>:76)\n",
       "$line14.$read$.<init>(<console>:80)\n",
       "$line14.$read$.<clinit>(<console>)\n",
       "$line14.$eval$.$print$lzycompute(<console>:7)\n",
       "$line14.$eval$.$print(<console>:6)\n",
       "$line14.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "StackTrace: org.apache.spark.SparkContext.<init>(SparkContext.scala:76)\n",
       "<init>(<console>:46)\n",
       "<init>(<console>:54)\n",
       "<init>(<console>:56)\n",
       "<init>(<console>:58)\n",
       "<init>(<console>:60)\n",
       "<init>(<console>:62)\n",
       "<init>(<console>:64)\n",
       "<init>(<console>:66)\n",
       "<init>(<console>:68)\n",
       "<init>(<console>:70)\n",
       "<init>(<console>:72)\n",
       "<init>(<console>:74)\n",
       "<init>(<console>:76)\n",
       ".<init>(<console>:80)\n",
       ".<clinit>(<console>)\n",
       ".$print$lzycompute(<console>:7)\n",
       ".$print(<console>:6)\n",
       "$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2483)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2479)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2479)\n",
       "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2568)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAppName(\"spark-test\").setMaster(\"local[*]\")\n",
    "val sc = new SparkContext(conf)\n",
    "val spark = SparkSession.builder().config(sc.getConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [cat: string, desc: string ... 4 more fields]\n",
       "film = [lang: string, id: bigint ... 2 more fields]\n",
       "filmLang = es\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "es"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"DO_record_per_line.json\")//.json(\"file.json\")\n",
    "import spark.implicits._\n",
    "val film = df.filter('id === 3879) //'id === 3879 or \n",
    "  .select('lang, 'id, 'name, regexp_replace('desc,lit(\"\"\"[\\p{Punct}]\"\"\"), lit(\" \")).alias(\"desc\"))\n",
    "\n",
    "val filmLang = film.collect().map(r=>r(0)).toList.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|  id|                name|                desc|\n",
      "+----+--------------------+--------------------+\n",
      "|  59|El ABC del empren...|A través de difer...|\n",
      "| 124|Pensamiento Cient...|Aprenderemos cómo...|\n",
      "| 160|   Ser más creativos|¡Claro que todos ...|\n",
      "| 166|Conceptos y Herra...|Este curso provee...|\n",
      "| 196|Egiptología (Egyp...|Este curso introd...|\n",
      "| 198|Continuidad y des...|¿Cuál es la impor...|\n",
      "| 252|Histología básica...|Este curso está d...|\n",
      "| 272|Desarrollo rápido...|Entender las meto...|\n",
      "| 273|Innovación educat...|Este curso promue...|\n",
      "| 386|        Pre-Calculus|Curso diseñado pa...|\n",
      "| 387|Matemáticas y Mov...|El curso propone ...|\n",
      "| 468|Tecnologías de in...|Analizar diferent...|\n",
      "|3660|Evaluación de Dec...| The Course Este ...|\n",
      "|3810|Diseña sitios web...| Aprende a crear ...|\n",
      "|3870|Apps de iOS y And...| Crea las apps qu...|\n",
      "|3872|Conozca las escal...| Desarrolle sus h...|\n",
      "|3873|Realice acompañam...| Aprenda a utiliz...|\n",
      "|3874|Conozca las escal...| Desarrolle sus h...|\n",
      "|3875|Realice acompañam...| Aprenda a utiliz...|\n",
      "|3876|Desarrolle al máx...| Comprenda y apre...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "corpus = [id: bigint, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, name: string ... 1 more field]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corpus = df.filter('lang === filmLang)\n",
    "  .select('id, 'name, regexp_replace('desc,lit(\"\"\"[\\p{Punct}]\"\"\"), lit(\" \")).alias(\"desc\"))\n",
    "corpus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:116: error: type mismatch;\n",
       " found   : Int(10000)\n",
       " required: String\n",
       "       val hashingTF = new HashingTF(10000).setInputCol(\"words\").setOutputCol(\"rawFeatures\")\n",
       "                                     ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer()\n",
    "  .setInputCol(\"desc\")\n",
    "  .setOutputCol(\"wordsR\")\n",
    "val wordsDataRaw = tokenizer.transform(corpus)\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(\"wordsR\")\n",
    "  .setOutputCol(\"words\")\n",
    "val wordsData = remover.transform(wordsDataRaw)\n",
    "\n",
    "val hashingTF = new HashingTF(10000).setInputCol(\"words\").setOutputCol(\"rawFeatures\")\n",
    "val featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf = idf_98c6af01e1dc\n",
       "idfModel = idf_98c6af01e1dc\n",
       "rescaledData = [id: bigint, name: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, name: string ... 5 more fields]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "val rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object udfUtils\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object udfUtils extends Serializable {\n",
    "    def calcNorm(vectorA: SparseVector): Double = {\n",
    "      var norm = 0.0\n",
    "      for (i <-  vectorA.indices){ norm += vectorA(i)*vectorA(i) }\n",
    "      if (norm > 0 ) (math.sqrt(norm)) else (0)\n",
    "    }\n",
    "    val calcNormDF = udf[Double,SparseVector](calcNorm)\n",
    "    \n",
    "    def cosineSimilarity(vectorA: SparseVector, vectorB:SparseVector,normASqrt:Double,normBSqrt:Double) :(Double) = {\n",
    "    var dotProduct = 0.0\n",
    "    for (i <-  vectorA.indices){ dotProduct += vectorA(i) * vectorB(i) }\n",
    "    val div = (normASqrt * normBSqrt)\n",
    "    if( div == 0 ) (0)\n",
    "    else (dotProduct / div)\n",
    "    }\n",
    "    \n",
    "    val calcCosineUDF = udf[Double,SparseVector,SparseVector,Double,Double](cosineSimilarity)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalized = [id: bigint, name: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, name: string ... 6 more fields]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val normalized = rescaledData.withColumn(\"norm\", udfUtils.calcNormDF(col(\"features\")))\n",
    "//normalized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|lang|  id|                desc|              wordsR|               words|         rawFeatures|           features2|            norm2|\n",
      "+----+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|  es|3879| Miguel uno de lo...|[, miguel, uno, d...|[, miguel, uno, d...|(262144,[335,2009...|(262144,[335,2009...|538.8291951140469|\n",
      "+----+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordsQueryRaw = [lang: string, id: bigint ... 3 more fields]\n",
       "wordsQuery = [lang: string, id: bigint ... 4 more fields]\n",
       "featurizedQuery = [lang: string, id: bigint ... 5 more fields]\n",
       "rescaledQuery = [lang: string, id: bigint ... 6 more fields]\n",
       "normalizedQuery = [lang: string, id: bigint ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[lang: string, id: bigint ... 6 more fields]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordsQueryRaw = tokenizer.transform(film)\n",
    "val wordsQuery = remover.transform(wordsQueryRaw)\n",
    "val featurizedQuery = hashingTF.transform(wordsQuery)\n",
    "val rescaledQuery = idfModel.transform(featurizedQuery)\n",
    "val normalizedQuery = rescaledQuery.withColumn(\"norm2\", udfUtils.calcNormDF(col(\"features\"))).withColumnRenamed(\"features\", \"features2\").drop(\"name\")\n",
    "normalizedQuery.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross = [lang: string, desc: string ... 13 more fields]\n",
       "cosine = [lang: string, desc: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[lang: string, desc: string ... 14 more fields]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cross = normalizedQuery.crossJoin(normalized).drop(normalizedQuery.col(\"id\"))\n",
    "val cosine = cross.withColumn(\"similarity\", udfUtils.calcCosineUDF(col(\"features\"), col(\"features2\"), col(\"norm\"), col(\"norm2\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "similarsDF = List(7602, 20593, 8832, 5218, 8727, 7173, 3683, 18884, 7172, 16718)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(7602, 20593, 8832, 5218, 8727, 7173, 3683, 18884, 7172, 16718)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val similarsDF = cosine.sort(desc(\"similarity\"),'name,'id).select(\"id\").limit(11).collect().takeRight(10).toList.map(r => r(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:64: error: object circe is not a member of package io\n",
       "       import io.circe.syntax._\n",
       "                 ^\n",
       "<console>:65: error: object circe is not a member of package io\n",
       "       import io.circe.generic.auto._\n",
       "                 ^\n",
       "<console>:66: error: value asJson is not a member of List[Any]\n",
       "       println(similarsDF.asJson)\n",
       "                          ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ret = Map(23325 -> similarsDF)\n",
    "import io.circe.syntax._\n",
    "import io.circe.generic.auto._\n",
    "println(similarsDF.asJson)\n",
    "val res = ret :: Nil\n",
    "//println(res.asJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
